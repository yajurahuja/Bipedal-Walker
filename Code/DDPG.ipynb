{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 3e-4):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise:\n",
    "    def __init__(self, action_space_size, mu = 0.0, sigma = 1.0, theta = 0.15):\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = None\n",
    "        self.action_space_size = action_space_size\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.mu * np.ones(self.action_space_size)\n",
    "    \n",
    "    def get_state(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.rand(self.action_space_size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        step = (state, action, np.array([reward]), next_state, done)\n",
    "        self.memory.append(step)\n",
    "    \n",
    "    def get_mini_batch(self, size):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        mini_batch = random.sample(self.memory, size)\n",
    "        for step in mini_batch:\n",
    "            state, action, reward, next_state, done = step\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,states_n, actions_n, hidden_size=256, actor_alpha=1e-4, critic_alpha=1e-3, gamma=0.99, tau=1e-2, max_memory_size=50000):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.actor_alpha = actor_alpha\n",
    "        self.critic_alpha = critic_alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_states = states_n\n",
    "        self.n_actions = actions_n\n",
    "        #create Actor Network\n",
    "        \n",
    "        self.Actor = Actor(self.n_states, self.hidden_size, self.n_actions)\n",
    "        self.Actor_t = Actor(self.n_states, self.hidden_size, self.n_actions)\n",
    "        for weight_t, weight in zip(self.Actor_t.parameters(), self.Actor.parameters()):\n",
    "            weight_t.data.copy_(weight.data)      \n",
    "        print(self.Actor.parameters()) \n",
    "        self.Actor_opt = optim.Adam(self.Actor.parameters(), lr=actor_alpha)\n",
    "        #create Critic Network\n",
    "        self.Critic = Critic(self.n_states + self.n_actions, hidden_size, self.n_actions)\n",
    "        self.Critic_t = Critic(self.n_states + self.n_actions, hidden_size, self.n_actions)\n",
    "        for weight_t, weight in zip(self.Critic_t.parameters(), self.Critic.parameters()):\n",
    "            weight_t.data.copy_(weight.data)\n",
    "        self.Critic_opt = optim.Adam(self.Critic.parameters(), lr=actor_alpha)\n",
    "        \n",
    "        #copy weights\n",
    "\n",
    "        \n",
    "            \n",
    "        #create noise\n",
    "        self.noise = Noise(self.n_actions)\n",
    "        #create buffer\n",
    "        self.replay_buffer = Buffer(max_memory_size)\n",
    "        \n",
    "    def push_buffer(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        #if buffer is full\n",
    "#         if(self.buffer.get_size() > batch_size):\n",
    "#             self.update(mini_batch_size)\n",
    "    \n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        #action = np.clip(action.numpy() + self.noise.get_state(), self.env.action_space.low, self.env.action_space.high)\n",
    "        action = self.Actor.forward(state)\n",
    "        action = action.detach().numpy()[0,0] + self.noise.get_state()\n",
    "        return action\n",
    "        \n",
    "    def update(self, mini_batch_size):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.get_mini_batch(mini_batch_size)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        \n",
    "        #update Critic\n",
    "        next_actions = self.Actor_t.forward(next_states)\n",
    "        Q_t_next = self.Critic_t.forward(next_states, next_actions)\n",
    "        Q_targets = rewards + (self.gamma * Q_t_next)\n",
    "        Q = self.Critic.forward(states, actions)\n",
    "        C_loss = F.mse_loss(Q, Q_targets)\n",
    "        self.Critic_opt.zero_grad()\n",
    "        C_loss.backward()\n",
    "        self.Critic_opt.step()\n",
    "        \n",
    "        #update Actor\n",
    "        new_actions = self.Actor.forward(states)\n",
    "        P_loss = -self.Critic.forward(states, new_actions).mean()\n",
    "        self.Actor_opt.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.Actor_opt.step()\n",
    "        \n",
    "        #update Target network\n",
    "        for target_param, param in zip(self.Actor_t.parameters(), self.Actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.Critic_t.parameters(), self.Critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "    def noise_reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def get_buffer(self):\n",
    "        return self.replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.74506630e-03,  1.52753142e-05, -1.99180283e-03, -1.60001028e-02,\n",
       "        9.26420316e-02,  4.62383591e-03,  8.59622747e-01, -2.27398394e-03,\n",
       "        1.00000000e+00,  3.29183005e-02,  4.62365011e-03,  8.53454545e-01,\n",
       "       -3.14610514e-03,  1.00000000e+00,  4.40813214e-01,  4.45819318e-01,\n",
       "        4.61421937e-01,  4.89549309e-01,  5.34101844e-01,  6.02459908e-01,\n",
       "        7.09147573e-01,  8.85930181e-01,  1.00000000e+00,  1.00000000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x10ececf68>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: -113.3, average _reward: nan \n",
      "episode: 1, reward: -115.12, average _reward: -113.29628317009534 \n",
      "episode: 2, reward: -115.13, average _reward: -114.20951914203872 \n",
      "episode: 3, reward: -114.86, average _reward: -114.51476134038826 \n",
      "episode: 4, reward: -115.8, average _reward: -114.6011160147203 \n",
      "episode: 5, reward: -119.87, average _reward: -114.84147369422858 \n",
      "episode: 6, reward: -112.81, average _reward: -115.67962503796696 \n",
      "episode: 7, reward: -115.97, average _reward: -115.2698633551923 \n",
      "episode: 8, reward: -115.85, average _reward: -115.35744932156135 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "o = env.observation_space.shape[0]\n",
    "a = env.action_space.shape[0]\n",
    "agent = Agent(o, a)\n",
    "\n",
    "for episode in range(50):\n",
    "    state = env.reset()\n",
    "    agent.noise_reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent.get_action(state)\n",
    "        env.render()\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "        agent.push_buffer(state, action, reward, new_state, done)\n",
    "        \n",
    "        if agent.get_buffer().get_size() > batch_size:\n",
    "            agent.update(batch_size)        \n",
    "        \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            sys.stdout.write(\"episode: {}, reward: {}, average _reward: {} \\n\".format(episode, np.round(episode_reward, decimals=2), np.mean(rewards[-10:])))\n",
    "            break\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
