{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import import_ipynb\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from Architecture import *\n",
    "from Noise import *\n",
    "from Buffer import *\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,states_n, actions_n, hidden_size=256, actor_alpha=1e-4, critic_alpha=1e-3, gamma=0.99, tau=1e-2, max_memory_size=50000):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.actor_alpha = actor_alpha\n",
    "        self.critic_alpha = critic_alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_states = states_n\n",
    "        self.n_actions = actions_n\n",
    "        #create Actor Network\n",
    "        \n",
    "        self.Actor = Actor(self.n_states, self.hidden_size, self.n_actions)\n",
    "        self.Actor_t = Actor(self.n_states, self.hidden_size, self.n_actions)\n",
    "        for weight_t, weight in zip(self.Actor_t.parameters(), self.Actor.parameters()):\n",
    "            weight_t.data.copy_(weight.data)      \n",
    "        print(self.Actor.parameters()) \n",
    "        #self.Actor_opt = optim.Adam(self.Actor.parameters(), lr=actor_alpha)\n",
    "        #create Critic Network\n",
    "        self.Critic = Critic(self.n_states + self.n_actions, hidden_size, self.n_actions)\n",
    "        self.Critic_t = Critic(self.n_states + self.n_actions, hidden_size, self.n_actions)\n",
    "        for weight_t, weight in zip(self.Critic_t.parameters(), self.Critic.parameters()):\n",
    "            weight_t.data.copy_(weight.data)\n",
    "        #self.Critic_opt = optim.Adam(self.Critic.parameters(), lr=actor_alpha)\n",
    "        \n",
    "        #copy weights\n",
    "\n",
    "        \n",
    "            \n",
    "        #create noise\n",
    "        self.noise = Noise(self.n_actions)\n",
    "        #create buffer\n",
    "        self.replay_buffer = Buffer(max_memory_size)\n",
    "        \n",
    "    def push_buffer(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "        #if buffer is full\n",
    "        if(self.buffer.get_size() > batch_size):\n",
    "            self.update(mini_batch_size)\n",
    "    \n",
    "        \n",
    "    def get_action(self, state, step):\n",
    "        state = torch.tensor(state.unsqueeze(0))\n",
    "        #action = np.clip(action.numpy() + self.noise.get_state(), self.env.action_space.low, self.env.action_space.high)\n",
    "        action = action.numpy() + self.noise.get_state()\n",
    "        return action\n",
    "        \n",
    "    def update(self, mini_batch_size):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.get_mini_batch(mini_batch_size)\n",
    "        \n",
    "        #update Critic\n",
    "        next_actions = self.Actor_t(next_states)\n",
    "        Q_t_next = self.Critic_t(next_states, next_actions)\n",
    "        Q_targets = rewards + (gamma * Q_t_next)\n",
    "        Q = self.Critic(states, actions)\n",
    "        C_loss = F.mse_loss(Q, Q_targets)\n",
    "        self.Critic_opt.zero_grad()\n",
    "        C_loss.backward()\n",
    "        self.Critic_opt.step()\n",
    "        \n",
    "        #update Actor\n",
    "        new_actions = self.Actor(states)\n",
    "        P_loss = -np.mean(self.Critic(states, new_actions))\n",
    "        self.Actor_opt.zero_grad()\n",
    "        P_loss.backward()\n",
    "        self.Actor_opt.step()\n",
    "        \n",
    "        #update Target network\n",
    "        for target_param, param in zip(self.Actor_t.parameters(), self.Actor.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "       \n",
    "        for target_param, param in zip(self.Critic_t.parameters(), self.Critic.parameters()):\n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
