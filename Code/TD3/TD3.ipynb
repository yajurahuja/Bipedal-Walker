{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        xu = xu.view(xu.size(0), -1)\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.Memory = []\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        step = (state, action, reward, next_state, done)\n",
    "        self.Memory.append(step)\n",
    "    \n",
    "    def get_mini_batch(self, batch_size):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        batch = random.sample(self.Memory, batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, max_action, actor_alpha = 1e-3, critic_alpha = 1e-3 ,gamma = 0.99, tau = 0.005, noise_c = 1.0, d = 2, sigma = 1, batch_size = 100):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_c = noise_c\n",
    "        self.actor_alpha = actor_alpha\n",
    "        self.critic_alpha = critic_alpha\n",
    "        self.max_action = max_action\n",
    "        self.d = d\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma \n",
    "        self.sigma = sigma\n",
    "        self.current_state = None\n",
    "        \n",
    "        #Actor Network\n",
    "        self.actor = Actor(env.observation_space.shape[0], env.action_space.shape[0], self.max_action)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.act_opt = optim.Adam(self.actor.parameters(), lr=actor_alpha)\n",
    "        \n",
    "        #Critic Network\n",
    "        self.critic = Critic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.crit_opt = optim.Adam(self.critic.parameters(), lr=critic_alpha)\n",
    "        \n",
    "        #Relay Buffer\n",
    "        self.replay_buffer = Buffer()\n",
    "        \n",
    "    def step(self, time_step):\n",
    "        action = self.actor.forward(torch.FloatTensor(self.current_state))\n",
    "        action = (action.detach().numpy().flatten() + np.random.normal(0, self.sigma, self.env.action_space.shape[0]))\n",
    "        action.clip(self.env.action_space.low, self.env.action_space.high)\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        self.current_state = next_state\n",
    "        self.replay_buffer.add(self.current_state, action, reward, next_state, done)\n",
    "        if self.replay_buffer.get_size() > self.batch_size:\n",
    "            self.update(time_step)\n",
    "        if done is True:\n",
    "            done, state = False, self.env.reset()\n",
    "            return reward, True\n",
    "        return reward, done\n",
    "   \n",
    "\n",
    "    def update(self, time_step):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.get_mini_batch(self.batch_size) \n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(1-np.array(dones))\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        #print(next_actions.detach().numpy().shape)\n",
    "        \n",
    "        noise = torch.FloatTensor(actions).data.normal_(0, 1) #tensor of size BATCH_SIZE x ACTION_SIZE\n",
    "        noise = noise.clamp(-self.noise_c, self.noise_c)\n",
    "#         print(type(noise), noise.shape)\n",
    "        next_actions = (next_actions + noise).clamp(-self.max_action, self.max_action)\n",
    "        #print(next_states.shape, next_actions.shape)\n",
    "        #update critic\n",
    "        q_1, q_2 = self.critic_target.forward(next_states, next_actions)\n",
    "        y = rewards + (self.gamma * torch.min(q_1, q_2)).detach()\n",
    "        \n",
    "        current_q1, current_q2 = self.critic.forward(states, actions)\n",
    "        loss_c = F.mse_loss(y, current_q1) + F.mse_loss(y, current_q2)\n",
    "        \n",
    "        self.crit_opt.zero_grad()\n",
    "        loss_c.backward()\n",
    "        self.crit_opt.step()\n",
    "        \n",
    "        if time_step % self.d == 0:\n",
    "            #update actor\n",
    "            action = self.actor.forward(states)\n",
    "            loss_a = -self.critic.Q1(states, action).mean() #compute loss\n",
    "            self.act_opt.zero_grad()\n",
    "            loss_a.backward()\n",
    "            \n",
    "            self.act_opt.step()\n",
    "            self.target_update(self.critic, self.critic_target)\n",
    "            self.target_update(self.actor, self.actor_target)\n",
    "\n",
    "    def target_update(self, model, model_target):\n",
    "        for theta, theta_ in zip(model.parameters(), model_target.parameters()):\n",
    "            update = self.tau * theta + (1 - self.tau)*theta_\n",
    "            theta_.data.copy_(update)\n",
    "            \n",
    "    def run_episode(self, current_state):\n",
    "        self.current_state = current_state\n",
    "        episode_done = False\n",
    "        self.env.render() #start the episode\n",
    "        rewards = []\n",
    "        timesteps = 0\n",
    "        for timesteps in range(100)\n",
    "            reward, episode_done = self.step(timesteps)\n",
    "            avg_rewards.append(reward)\n",
    "            if episode_done is True:\n",
    "                break\n",
    "\n",
    "        return rewards\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "current_state = env.reset()\n",
    "env.observation_space.shape[0] + env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 avg reward nan\n",
      "Episode 1 avg reward nan\n",
      "Episode 2 avg reward nan\n",
      "Episode 3 avg reward nan\n",
      "Episode 4 avg reward nan\n",
      "Episode 5 avg reward nan\n",
      "Episode 6 avg reward nan\n",
      "Episode 7 avg reward nan\n",
      "Episode 8 avg reward nan\n",
      "Episode 9 avg reward nan\n",
      "Episode 10 avg reward nan\n",
      "Episode 11 avg reward nan\n",
      "Episode 12 avg reward nan\n",
      "Episode 13 avg reward nan\n",
      "Episode 14 avg reward nan\n",
      "Episode 15 avg reward nan\n",
      "Episode 16 avg reward nan\n",
      "Episode 17 avg reward nan\n",
      "Episode 18 avg reward nan\n",
      "Episode 19 avg reward nan\n",
      "Episode 20 avg reward nan\n",
      "Episode 21 avg reward nan\n",
      "Episode 22 avg reward nan\n",
      "Episode 23 avg reward nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:62: UserWarning: Using a target size (torch.Size([100, 1])) that is different to the input size (torch.Size([100, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 avg reward nan\n",
      "Episode 25 avg reward nan\n",
      "Episode 26 avg reward nan\n",
      "Episode 27 avg reward nan\n",
      "Episode 28 avg reward nan\n",
      "Episode 29 avg reward nan\n",
      "Episode 30 avg reward nan\n",
      "Episode 31 avg reward nan\n",
      "Episode 32 avg reward nan\n",
      "Episode 33 avg reward nan\n",
      "Episode 34 avg reward nan\n",
      "Episode 35 avg reward nan\n",
      "Episode 36 avg reward nan\n",
      "Episode 37 avg reward nan\n",
      "Episode 38 avg reward nan\n",
      "Episode 39 avg reward nan\n",
      "Episode 40 avg reward nan\n",
      "Episode 41 avg reward nan\n",
      "Episode 42 avg reward nan\n",
      "Episode 43 avg reward nan\n",
      "Episode 44 avg reward nan\n",
      "Episode 45 avg reward nan\n",
      "Episode 46 avg reward nan\n",
      "Episode 47 avg reward nan\n",
      "Episode 48 avg reward nan\n",
      "Episode 49 avg reward nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = []\n",
    "avg_rewards = []\n",
    "max_action = float(env.action_space.high[0])\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.1\n",
    "rewards = []\n",
    "env.observation_space.shape[0]\n",
    "agent = Agent(env, max_action)\n",
    "for episode in range(50):\n",
    "    \n",
    "    episode_rewards = agent.run_episode(current_state)\n",
    "    \n",
    "    print('Episode '+str(episode) + \" avg reward \"+ str(np.mean(episode_rewards)))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
