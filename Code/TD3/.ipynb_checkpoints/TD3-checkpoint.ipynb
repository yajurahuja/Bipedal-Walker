{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        xu = xu.view(xu.size(0), -1)\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.Memory = []\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        step = (state, action, reward, next_state, done)\n",
    "        self.Memory.append(step)\n",
    "    \n",
    "    def get_mini_batch(self, batch_size):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        batch = random.sample(self.Memory, batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, max_action, actor_alpha = 1e-3, critic_alpha = 1e-3 ,gamma = 0.99, tau = 0.005, noise_c = 1.0, d = 2, sigma = 1, batch_size = 100):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_c = noise_c\n",
    "        self.actor_alpha = actor_alpha\n",
    "        self.critic_alpha = critic_alpha\n",
    "        self.max_action = max_action\n",
    "        self.d = d\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma \n",
    "        self.sigma = sigma\n",
    "        self.current_state = None\n",
    "        \n",
    "        #Actor Network\n",
    "        self.actor = Actor(env.observation_space.shape[0], env.action_space.shape[0], self.max_action)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.act_opt = optim.Adam(self.actor.parameters(), lr=actor_alpha)\n",
    "        \n",
    "        #Critic Network\n",
    "        self.critic = Critic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.crit_opt = optim.Adam(self.critic.parameters(), lr=critic_alpha)\n",
    "        \n",
    "        #Relay Buffer\n",
    "        self.replay_buffer = Buffer()\n",
    "        \n",
    "    def step(self, time_step):\n",
    "        action = self.actor.forward(torch.FloatTensor(self.current_state))\n",
    "        action = (action.detach().numpy().flatten() + np.random.normal(0, self.sigma, self.env.action_space.shape[0]))\n",
    "        action.clip(self.env.action_space.low, self.env.action_space.high)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        print(reward)\n",
    "        self.replay_buffer.add(self.current_state, action, reward, next_state, done)\n",
    "        if self.replay_buffer.get_size() > self.batch_size:\n",
    "            self.update(time_step)\n",
    "        self.current_state = next_state\n",
    "        if done is True:\n",
    "            done, state = False, self.env.reset()\n",
    "            return reward, True\n",
    "        return reward, done\n",
    "   \n",
    "\n",
    "    def update(self, time_step):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.get_mini_batch(self.batch_size) \n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(1-np.array(dones))\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        #print(next_actions.detach().numpy().shape)\n",
    "        \n",
    "        noise = torch.FloatTensor(actions).data.normal_(0, 1) #tensor of size BATCH_SIZE x ACTION_SIZE\n",
    "        noise = noise.clamp(-self.noise_c, self.noise_c)\n",
    "#         print(type(noise), noise.shape)\n",
    "        next_actions = (next_actions + noise).clamp(-self.max_action, self.max_action)\n",
    "        #print(next_states.shape, next_actions.shape)\n",
    "        #update critic\n",
    "        q_1, q_2 = self.critic_target.forward(next_states, next_actions)\n",
    "        y = rewards + (self.gamma * torch.min(q_1, q_2)).detach()\n",
    "        \n",
    "        current_q1, current_q2 = self.critic.forward(states, actions)\n",
    "        loss_c = F.mse_loss(y, current_q1) + F.mse_loss(y, current_q2)\n",
    "        \n",
    "        self.crit_opt.zero_grad()\n",
    "        loss_c.backward()\n",
    "        self.crit_opt.step()\n",
    "        \n",
    "        if time_step % self.d == 0:\n",
    "            #update actor\n",
    "            action = self.actor.forward(states)\n",
    "            loss_a = -self.critic.Q1(states, action).mean() #compute loss\n",
    "            self.act_opt.zero_grad()\n",
    "            loss_a.backward()\n",
    "            \n",
    "            self.act_opt.step()\n",
    "            self.target_update(self.critic, self.critic_target)\n",
    "            self.target_update(self.actor, self.actor_target)\n",
    "\n",
    "    def target_update(self, model, model_target):\n",
    "        for theta, theta_ in zip(model.parameters(), model_target.parameters()):\n",
    "            update = self.tau * theta + (1 - self.tau)*theta_\n",
    "            theta_.data.copy_(update)\n",
    "            \n",
    "    def run_episode(self):\n",
    "        self.current_state = env.reset()\n",
    "        episode_done = False\n",
    "        rewards = []\n",
    "        timesteps = 0\n",
    "        for timesteps in range(10000):\n",
    "            reward, episode_done = self.step(timesteps)\n",
    "            self.env.render() #start the episode\n",
    "            rewards.append(reward)\n",
    "            if episode_done is True:\n",
    "                break\n",
    "\n",
    "        return rewards\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "current_state = env.reset()\n",
    "env.observation_space.shape[0] + env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2297622591540184\n",
      "-0.2740298466331524\n",
      "-0.20103112808073167\n",
      "-0.18026637118298885\n",
      "-0.25303377640591096\n",
      "-0.1374244294896588\n",
      "-0.05732720965938798\n",
      "0.0266958860616943\n",
      "-0.10061432305354355\n",
      "-0.060447835267726804\n",
      "0.034651671660803275\n",
      "-0.19901980878408349\n",
      "-0.15298361850232217\n",
      "-0.2888054114813127\n",
      "-0.2693366413117559\n",
      "-0.1663716454864888\n",
      "-0.364226587600416\n",
      "0.08733632766086749\n",
      "0.11088830858228507\n",
      "0.02783165714032662\n",
      "0.16254986893687287\n",
      "-0.02823024888445984\n",
      "0.0647227451479658\n",
      "0.06982192514754088\n",
      "0.17047246922130382\n",
      "0.1288603573139789\n",
      "0.07261433144987531\n",
      "-0.2058313108672682\n",
      "-0.21309257149962268\n",
      "-0.21685234404237638\n",
      "-0.2246659401388693\n",
      "-0.245511359790501\n",
      "-0.3107298872033085\n",
      "-0.31279580873040636\n",
      "-0.2610969136390081\n",
      "-0.2903402178552307\n",
      "-0.19772248837819995\n",
      "-0.1313121311181995\n",
      "-0.12869555438458216\n",
      "-0.1177618449568942\n",
      "-0.177045486826978\n",
      "-0.2037727174163357\n",
      "-0.13840896608860082\n",
      "-0.1411371065308806\n",
      "-0.08720699058698558\n",
      "-0.03078910019529176\n",
      "-0.05273963251347825\n",
      "-0.03273653781921409\n",
      "-0.07678804356625601\n",
      "-0.2409702009610682\n",
      "-0.23242350865726982\n",
      "-0.23972312895460224\n",
      "-0.2111417156996102\n",
      "-0.14172654960565648\n",
      "-0.1508781343046689\n",
      "-0.1347504226838651\n",
      "-0.23899920686480833\n",
      "-0.2553298433872868\n",
      "-0.34144137794737445\n",
      "-0.36041209085002374\n",
      "-0.2872446294985174\n",
      "-0.33361495439151256\n",
      "-0.25323408014249366\n",
      "-0.2396800018427576\n",
      "-0.2412008316560516\n",
      "-0.26256059467646997\n",
      "-0.3129276554090736\n",
      "-0.23257510789910293\n",
      "-0.25481924216323154\n",
      "-0.3052798110574379\n",
      "-0.25428605248259034\n",
      "-0.38584196633973467\n",
      "-0.3908033598746336\n",
      "-0.26984260004359695\n",
      "-0.19715550071582652\n",
      "-0.3707683915234741\n",
      "-0.26602862030811114\n",
      "-0.15515235635223604\n",
      "-0.10493208958975601\n",
      "-0.20731560517448053\n",
      "-0.0719664587756184\n",
      "-0.040731587743581585\n",
      "-0.006530857832427685\n",
      "-0.06114283139916127\n",
      "-0.026207567673746962\n",
      "-0.020227331405864107\n",
      "-0.20475020326339646\n",
      "-0.2242692796970896\n",
      "-0.3964147170636706\n",
      "-0.23746871980499973\n",
      "-0.40330754494295973\n",
      "-0.2650792694547243\n",
      "-0.25903951869764824\n",
      "-0.1611407094631457\n",
      "-0.12902260560514078\n",
      "-0.14507400625501582\n",
      "-0.18523052273678522\n",
      "-0.16714659926710052\n",
      "-0.2637094346027851\n",
      "-0.3177042746975082\n",
      "Episode 0 avg reward -17.26272221824262\n",
      "-0.1539155635837466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: Using a target size (torch.Size([100, 1])) that is different to the input size (torch.Size([100, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0853865484382535\n",
      "-0.2039717715505403\n",
      "-0.17701176114794911\n",
      "-0.12075855483262338\n",
      "-0.04342754107815362\n",
      "-0.010502061648142096\n",
      "-0.08804079042117877\n",
      "-0.2568275584893148\n",
      "-0.142024689560522\n",
      "-0.10326982083024049\n",
      "-0.0712046378191995\n",
      "-0.12968547133715952\n",
      "-0.129992999759863\n",
      "-0.14505752344503847\n",
      "-0.05062956284540988\n",
      "-0.13584504743695142\n",
      "-0.08357403295818366\n",
      "-0.07066429192325382\n",
      "-0.13466123604911212\n",
      "-0.0954577992529255\n",
      "0.04256387839344347\n",
      "-0.046066403212364514\n",
      "0.0033144281385271354\n",
      "-0.0056535096358014715\n",
      "0.07222551645337202\n",
      "0.0014538204660341468\n",
      "0.09065544268054343\n",
      "0.19070885263198487\n",
      "0.1297851979036688\n",
      "0.06369407245249924\n",
      "0.0959495645874816\n",
      "0.0650649424688772\n",
      "0.10288456283120351\n",
      "0.09639248597621919\n",
      "0.10801254570331825\n",
      "0.18092900847446236\n",
      "0.10420167024778465\n",
      "-0.1471258197290638\n",
      "0.042614648917953274\n",
      "0.07675394486682229\n",
      "-0.06886937471942209\n",
      "0.034704770184037476\n",
      "0.08363922906119016\n",
      "-0.05064506691437247\n",
      "-0.0012859377825423027\n",
      "-0.06693160480143237\n",
      "-0.32204131125479535\n",
      "-0.05529429582372083\n",
      "-0.10844686130142572\n",
      "-0.07696485663156917\n",
      "-0.06647021647210977\n",
      "-0.052978750375837366\n",
      "-0.13828910167789726\n",
      "-0.16980816527767714\n",
      "-0.12736591746364126\n",
      "-0.20497945860377614\n",
      "-0.05402322016159457\n",
      "0.013914263883430715\n",
      "0.06583989967859966\n",
      "0.147510354449354\n",
      "0.12071922262994132\n",
      "-0.0786551531108212\n",
      "-0.08886457130405072\n",
      "-0.07179158184473997\n",
      "-0.11079055133809185\n",
      "-0.00803508864632661\n",
      "0.004558732630966556\n",
      "-0.02405694943437057\n",
      "-0.15682080691595301\n",
      "-0.08466384348451995\n",
      "-0.057298835837903614\n",
      "-0.04975712082374019\n",
      "-0.04420753941601516\n",
      "0.13814752972742855\n",
      "0.02390262455375641\n",
      "0.07323107219482305\n",
      "0.09512726695313709\n",
      "0.1450140342128115\n",
      "-0.2683290285688762\n",
      "-0.16443412066517774\n",
      "-0.28440162194535323\n",
      "-0.27125785856312706\n",
      "-0.2056911030874715\n",
      "-0.2925334226121339\n",
      "-0.23523370946716735\n",
      "-0.14934784922878802\n",
      "-0.1492078437485535\n",
      "-0.15291307110133445\n",
      "-0.08694396217117617\n",
      "-0.16432964881432385\n",
      "-0.11447697533507385\n",
      "-0.23710417070909903\n",
      "-0.22264804304257088\n",
      "-0.05917293707102623\n",
      "0.0458135845435569\n",
      "0.11194310077687414\n",
      "0.03892300368891412\n",
      "0.07703959511607539\n",
      "7.911774922682996e-05\n",
      "Episode 1 avg reward -5.340804559306271\n",
      "-0.15282509267386063\n",
      "-0.17682165046410425\n",
      "-0.17537492652709047\n",
      "-0.12557791290622597\n",
      "-0.14544606111446778\n",
      "-0.07216311356602463\n",
      "-0.08109834723417449\n",
      "-0.12013925841820589\n",
      "-0.05999360415144839\n",
      "-0.09453234161633761\n",
      "-0.10753223675492206\n",
      "-0.05450175444831748\n",
      "-0.04240949298464101\n",
      "-0.12946578450387677\n",
      "-0.1795971378110766\n",
      "-0.15614024599535112\n",
      "-0.20762935238049798\n",
      "-0.08758427998123193\n",
      "-0.07171215465583437\n",
      "-0.003332181691167533\n",
      "-0.045879445521190845\n",
      "-0.005574956919325847\n",
      "-0.03118671275468366\n",
      "-0.020664166603516502\n",
      "-0.022799701961815085\n",
      "0.07484573480935933\n",
      "0.05353858957733082\n",
      "0.09336790788173677\n",
      "0.1848518677813833\n",
      "0.2865736418510665\n",
      "0.24180086363786402\n",
      "0.2358824732261309\n",
      "0.29087320222409047\n",
      "0.19471125417493668\n",
      "0.15578474232945155\n",
      "0.15804936137864137\n",
      "0.008150330759185434\n",
      "-0.20657258261578434\n",
      "-0.20558153794631992\n",
      "-0.06794295114278794\n",
      "-0.14757230953213035\n",
      "-0.08036547538661092\n",
      "-0.10330333879653827\n",
      "-0.04921338038937051\n",
      "0.012738882833098455\n",
      "0.06407950449310394\n",
      "0.10537045557388477\n",
      "0.1709669554463946\n",
      "-0.056466163941178\n",
      "-0.054720952534583464\n",
      "-0.05712981004510642\n",
      "-0.07738158211492124\n",
      "-0.09381166518342211\n",
      "-0.16534986846598662\n",
      "-0.18011412354751422\n",
      "-0.14938746499829386\n",
      "-0.15943175049417874\n",
      "-0.1360981000354232\n",
      "-0.05447189655962152\n",
      "-0.11858241713300262\n",
      "-0.12412669732944999\n",
      "-0.1289949819314066\n",
      "-0.013442996487404785\n",
      "-0.09488737824306066\n",
      "-0.06831344333794406\n",
      "-0.04861164570279604\n",
      "-0.06450647057520349\n",
      "-0.026020500783117947\n",
      "-0.0818679190467103\n",
      "-0.051493196445331936\n",
      "-0.07827779197210775\n",
      "-0.08362335902682341\n",
      "-0.04762355652950566\n",
      "-0.08597950267837118\n",
      "-0.09043597728817891\n",
      "-0.07499569110935692\n",
      "-0.1064802560902674\n",
      "-0.10187723458246209\n",
      "-0.07000097569493657\n",
      "-0.07957252080314682\n",
      "-0.05508377943707205\n",
      "-0.07165660123662813\n",
      "-0.05884184292540252\n",
      "-0.06713204361089001\n",
      "-0.0789589176774025\n",
      "-0.02553233488552281\n",
      "-0.024634357615607377\n",
      "-0.09800326203498398\n",
      "0.058345469509564964\n",
      "-0.055657963123546234\n",
      "-0.04512300933026969\n",
      "-0.017080837312994833\n",
      "-0.02606388182853008\n",
      "-0.002170569719434309\n",
      "-0.06894497097471237\n",
      "-0.08788208839279696\n",
      "-0.12893139451778848\n",
      "-0.1316770250629192\n",
      "-0.07024477210014443\n",
      "-0.15164583824078004\n",
      "Episode 2 avg reward -4.9299456326999485\n",
      "-0.146489841242463\n",
      "-0.3526932523382852\n",
      "-0.24483941317357058\n",
      "-0.28484963052319995\n",
      "-0.19493566306458834\n",
      "-0.21046042920463365\n",
      "-0.3330169191766644\n",
      "-0.19935221286670962\n",
      "-0.267310410878467\n",
      "-0.1998323944791894\n",
      "-0.10072722464743972\n",
      "-0.037027589239107726\n",
      "-0.14805043571830906\n",
      "-0.1777022349380047\n",
      "-0.10600472300258004\n",
      "-0.14977600109039071\n",
      "-0.06966147610245876\n",
      "-0.21940549592820735\n",
      "-0.2068690993511279\n",
      "-0.19041352377198315\n",
      "-0.19077532593288948\n",
      "-0.3427919898627185\n",
      "-0.36248559076009007\n",
      "-0.40087829429344934\n",
      "-0.4226666978890225\n",
      "-0.21530007321124547\n",
      "-0.2648806958211154\n",
      "-0.11522513054745649\n",
      "-0.11714727830998911\n",
      "-0.1194854315648998\n",
      "-0.03683410112672079\n",
      "-0.08586403630474987\n",
      "0.025173435455535423\n",
      "-0.10337646753433037\n",
      "-0.11231992802653556\n",
      "-0.05726220092159574\n",
      "-0.08435082586606225\n",
      "-0.035857785133717915\n",
      "-0.06494006270413914\n",
      "-0.03576575128649767\n",
      "-0.10645916610346423\n",
      "-0.06285092882315436\n",
      "-0.04342340783515142\n",
      "-0.08370995118615605\n",
      "-0.10383328201373655\n",
      "-0.06412189860394761\n",
      "-0.06491215852870501\n",
      "-0.037574159595703205\n",
      "-0.1526042300266532\n",
      "-0.12373841518660622\n",
      "-0.10612751694520196\n",
      "-0.20639380638213514\n",
      "-0.2139880339616414\n",
      "-0.17397215089738663\n",
      "-0.1215770692590439\n",
      "-0.08359177661985479\n",
      "-0.16347986318885716\n",
      "-0.1453149622312782\n",
      "-0.13559401703422105\n",
      "-0.09611230396823228\n",
      "-0.08407419822039716\n",
      "-0.06814342166018499\n",
      "-0.042000430443559285\n",
      "-0.048152336032896634\n",
      "-0.12102235208642852\n",
      "-0.039566441466587714\n",
      "-0.05201532452439099\n",
      "-0.030563328111054708\n",
      "-0.051049673632340045\n",
      "-0.02306965679191778\n",
      "0.01041043968268032\n",
      "0.010794428495701999\n",
      "-0.044156086488511506\n",
      "-0.04227406475540814\n",
      "-0.11215963392800281\n",
      "-0.15147873063380685\n",
      "-0.08307151990593044\n",
      "-0.11296370652318001\n",
      "-0.10264147731040435\n",
      "-0.11075898164487369\n",
      "-0.1378125191223138\n",
      "-0.12014118722373339\n",
      "-0.04761368626823739\n",
      "-0.1382462275265194\n",
      "-0.0025800261268620773\n",
      "0.04374976266953677\n",
      "-0.16325715828006923\n",
      "-0.08914073883432824\n",
      "0.05926723108244353\n",
      "-0.041765246191149887\n",
      "-0.14332055356226758\n",
      "-0.23298092159424302\n",
      "-0.19314120560993278\n",
      "-0.08492996942996979\n",
      "-0.08768158118615524\n",
      "-0.04653006101615866\n",
      "-0.08686557313517988\n",
      "-0.08384659270366142\n",
      "-0.05930657791882277\n",
      "-0.038123479769706425\n",
      "Episode 3 avg reward -12.264054108499026\n",
      "-0.06413660160647573\n",
      "-0.29824203722835274\n",
      "-0.3316495868180823\n",
      "-0.3151876207833263\n",
      "-0.22653456209445516\n",
      "-0.2468017916518349\n",
      "-0.20570904920900807\n",
      "-0.09768406662318438\n",
      "-0.06905223555920972\n",
      "0.07721467881458982\n",
      "0.08168161631998422\n",
      "0.06949210777603879\n",
      "0.13631067324480425\n",
      "0.09757103317983967\n",
      "-0.33240367169351565\n",
      "-0.29115318486609154\n",
      "-0.2674888787149969\n",
      "-0.34813805339158804\n",
      "-0.3511887859439691\n",
      "-0.3925786929594257\n",
      "-0.45416049717834567\n",
      "-0.40484964224625175\n",
      "-0.3343376521149341\n",
      "-0.3952249850452985\n",
      "-0.37654877788003666\n",
      "-0.3387112915706364\n",
      "-0.24778760838623848\n",
      "-0.20370111020406323\n",
      "-0.17336578555623655\n",
      "-0.13457123530622483\n",
      "-0.1247744174997283\n",
      "-0.1846073190192891\n",
      "-0.00549710359467967\n",
      "-0.06983825715382776\n",
      "0.0033920692932107553\n",
      "-0.01010672888553546\n",
      "-0.05734909151820394\n",
      "-0.008560627704866243\n",
      "-0.16115770387021466\n",
      "-0.22139066759235834\n",
      "-0.05041807853112287\n",
      "0.026871306672351086\n",
      "0.05696325386145443\n",
      "0.05750724188013886\n",
      "-0.008412019800361971\n",
      "-0.07947160932951189\n",
      "-0.028435174485775695\n",
      "-0.06490474302063491\n",
      "-0.06844269442295159\n",
      "-0.07824001154836006\n",
      "-0.09239805892998272\n",
      "-0.08325215254813241\n",
      "-0.07150397132209398\n",
      "-0.05792902409759156\n",
      "-0.08081948348147824\n",
      "-0.0855258986444243\n",
      "0.0011822009838825462\n",
      "0.10989139766984549\n",
      "0.047193114688743557\n",
      "0.00210617140632963\n",
      "0.003929979061525941\n",
      "0.00691744045430975\n",
      "-0.01615205083485056\n",
      "0.10869318434441219\n",
      "-0.02500496992807029\n",
      "-0.07159566665805928\n",
      "0.056420712070434476\n",
      "-0.11768368037541588\n",
      "-0.08473108620420719\n",
      "-0.14014060567898706\n",
      "0.012258697032928475\n",
      "0.10579091688438534\n",
      "0.2027011222304687\n",
      "0.08470336984149487\n",
      "-0.08196235152513823\n",
      "0.06444288613413088\n",
      "0.0016489303359462568\n",
      "0.017566968376278382\n",
      "0.1323259128219752\n",
      "0.20755292820930477\n",
      "0.14199805062026197\n",
      "0.10602214275768013\n",
      "0.013626068403297588\n",
      "0.04397351345187922\n",
      "0.14413259844658535\n",
      "0.017281600316364384\n",
      "0.07206211390103441\n",
      "0.16520314529390986\n",
      "0.13601097797830228\n",
      "0.19777168088499986\n",
      "0.3032879165741898\n",
      "0.2591170251224942\n",
      "0.11164066965370481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12798271537274822\n",
      "0.040070862592996116\n",
      "-0.08281194967080462\n",
      "-0.2502412185490188\n",
      "-0.2650647986316705\n",
      "-0.25423758028188215\n",
      "-0.23199729654356233\n",
      "Episode 4 avg reward -6.5633545015553185\n",
      "-0.13152942754728317\n",
      "-0.1536866308934753\n",
      "-0.10716831341051358\n",
      "-0.1506156375408475\n",
      "-0.24377919735594514\n",
      "-0.29220991077296116\n",
      "-0.3629989539782219\n",
      "-0.33507793275244124\n",
      "-0.31811491656450885\n",
      "-0.2267650565258425\n",
      "-0.28071989676694054\n",
      "-0.2580209117450131\n",
      "-0.3214756082405587\n",
      "-0.1090334377992828\n",
      "-0.09396737312647085\n",
      "-0.08863828423123896\n",
      "-0.12433386993408202\n",
      "-0.07290952454089987\n",
      "-0.05997757082687066\n",
      "-0.03560641856327785\n",
      "0.07752116839538219\n",
      "0.16250892720942522\n",
      "0.13600951639103145\n",
      "0.17862412182489792\n",
      "0.22283518955273132\n",
      "0.24606893889880466\n",
      "0.27832148867123424\n",
      "0.1370352745944083\n",
      "-0.3360225407629967\n",
      "-0.4161930218656578\n",
      "-0.4183003992040977\n",
      "-0.3314633536175324\n",
      "-0.18694860883998418\n",
      "-0.20232739651828924\n",
      "-0.3093846687404416\n",
      "-0.29293573355086105\n",
      "-0.282817272240683\n",
      "-0.47188396406173716\n",
      "-0.4416681528526361\n",
      "-0.4571973370979417\n",
      "-0.4406314677051469\n",
      "-0.44477067091932987\n",
      "-0.473001582945848\n",
      "-0.3937237101713028\n",
      "-0.47534613958994676\n",
      "-0.35788425642063126\n",
      "-0.45961037848370295\n",
      "-0.44335149077626246\n",
      "-0.48582904688517026\n",
      "-0.3480319078025685\n",
      "-0.3031988288877067\n",
      "-0.1362610047661699\n",
      "-0.1937289299809775\n",
      "-0.18322095254529747\n",
      "-0.2880165229749317\n",
      "-0.3827069351579616\n",
      "-0.3093072720938666\n",
      "-0.23815074185119067\n",
      "-0.25404314435275366\n",
      "-0.22862281934125947\n",
      "-0.1795709251283139\n",
      "-0.26074393634145826\n",
      "-0.27780555364219744\n",
      "-0.3035470391623188\n",
      "-0.34860646344760543\n",
      "-0.16554427486536588\n",
      "-0.33710795516807124\n",
      "-0.2700522298489498\n",
      "-0.25668580276563663\n",
      "-0.20363924687049592\n",
      "-100\n",
      "Episode 5 avg reward -115.91758792782406\n",
      "-0.09960034406728298\n",
      "-0.15310602611649868\n",
      "-0.1430941361786719\n",
      "-0.2460575011504795\n",
      "-0.1681222305183504\n",
      "-0.13463533265784455\n",
      "-0.18414033577147101\n",
      "-0.17464923469561514\n",
      "-0.17444687856368607\n",
      "-0.15121377073185266\n",
      "-0.14964256298674752\n",
      "-0.1972938549434278\n",
      "-0.14802985254668677\n",
      "-0.02583715870990656\n",
      "0.062335687154217\n",
      "0.03294487259971421\n",
      "-0.026718967545275658\n",
      "0.039309438049793255\n",
      "0.10220926878587455\n",
      "0.1308012234762781\n",
      "0.09896636108476825\n",
      "-0.3393975269534713\n",
      "-0.43770678879160113\n",
      "-0.46452273501976105\n",
      "-0.4367486168488215\n",
      "-0.39025115388982096\n",
      "-0.4063450131862625\n",
      "-0.2092649653752633\n",
      "-0.091356792445214\n",
      "-0.09715933849224569\n",
      "-0.15478543927349517\n",
      "-0.1626566223540097\n",
      "-0.1933257783255038\n",
      "-0.1835781139551917\n",
      "-0.1051457920223465\n",
      "-0.11828802116318253\n",
      "-0.28646994998551323\n",
      "-0.22728379136789303\n",
      "-0.15828451826060042\n",
      "-0.09007706247420078\n",
      "-0.24108533896487463\n",
      "-0.23617861406181165\n",
      "-0.342404236043462\n",
      "-0.22662045684914778\n",
      "-0.3819259101543641\n",
      "-0.3780298619362193\n",
      "-0.33557732790996375\n",
      "-0.31572948669715845\n",
      "-0.35486424865392985\n",
      "-0.2708379534272067\n",
      "-0.19959565246312316\n",
      "-0.149439146630595\n",
      "-100\n",
      "Episode 6 avg reward -109.69495759000941\n",
      "-0.0736331425991099\n",
      "-0.18872974458504688\n",
      "-0.2669872066928521\n",
      "-0.14844388971537406\n",
      "-0.07113581629025886\n",
      "-0.022223725902371677\n",
      "-0.0659868514960154\n",
      "-0.08907282426163132\n",
      "-0.19062946375753584\n",
      "-0.15184266748188915\n",
      "-0.15577265898139683\n",
      "-0.128220471315108\n",
      "-0.1791813749746009\n",
      "-0.18372547430311764\n",
      "-0.25601631299202443\n",
      "-0.32344474300643405\n",
      "-0.3484160657788815\n",
      "-0.26808127210574617\n",
      "-0.31824943499111635\n",
      "-0.23837580326630223\n",
      "-0.29555353503343895\n",
      "-0.32321051672298123\n",
      "-0.2877356284478628\n",
      "-0.1786775160890594\n",
      "-0.12318102162899762\n",
      "-0.16949438484351215\n",
      "-0.10587923819355538\n",
      "-0.31945835701624675\n",
      "-0.331879726204357\n",
      "-0.23996374414025048\n",
      "-0.31776249378611715\n",
      "-0.2770423472570735\n",
      "-0.1876526371614605\n",
      "-0.27662103751109024\n",
      "-0.22286268000696724\n",
      "-0.26512275363933924\n",
      "-0.3001760580281952\n",
      "-0.22880565761450453\n",
      "-0.28631787811778553\n",
      "-0.30154062229230694\n",
      "-0.30057752115329733\n",
      "-0.30875560792287077\n",
      "-0.09212650803059008\n",
      "-0.002411818557953225\n",
      "-0.06463266040615287\n",
      "-0.014792116269368173\n",
      "-0.029747773329132653\n",
      "-0.023175653270285423\n",
      "-0.007975690348786504\n",
      "-0.026218311185226887\n",
      "0.006623312469562992\n",
      "-0.012964158161583798\n",
      "0.03520445783092156\n",
      "0.05375968178614021\n",
      "-0.04866865251941542\n",
      "0.01352836357570492\n",
      "-0.03978027675815132\n",
      "0.028429500802041718\n",
      "0.019675449020815302\n",
      "-0.15177100054423132\n",
      "-0.028351873462107876\n",
      "-0.004866703670250522\n",
      "0.035897905015481096\n",
      "0.0355564973813549\n",
      "-0.031242528365112376\n",
      "-0.0165435936011681\n",
      "0.030997265743268877\n",
      "0.03753866506108257\n",
      "0.03835759486208594\n",
      "0.02958350752636783\n",
      "0.16679329528477652\n",
      "0.2035729865449092\n",
      "0.2453670404510192\n",
      "0.14967172224634046\n",
      "0.11297371548794624\n",
      "0.18545774050853644\n",
      "0.2645203810782759\n",
      "0.3090316920307642\n",
      "0.2110196100882276\n",
      "0.26327514105912225\n",
      "0.21005193581285483\n",
      "0.27363563979408606\n",
      "0.30701260399964125\n",
      "0.21357612889604763\n",
      "0.24488907135838206\n",
      "0.23330733671089907\n",
      "0.14902454392451098\n",
      "0.030156656155217936\n",
      "0.14764236462880814\n",
      "0.11786460936293228\n",
      "0.20577094791735082\n",
      "0.23263391252573276\n",
      "0.17504194234555862\n",
      "-0.24161876415468558\n",
      "-0.4447318024195536\n",
      "-0.4154241980999145\n",
      "-0.4038412091695612\n",
      "-0.376527757002346\n",
      "-0.3042557530071972\n",
      "-0.36544137370361446\n",
      "Episode 7 avg reward -7.446106864057706\n",
      "-0.25609179699695567\n",
      "-0.049982289941842985\n",
      "-0.1303616449487199\n",
      "-0.06938638274356497\n",
      "-0.16335382031398626\n",
      "-0.15395733034386355\n",
      "-0.16376816479882003\n",
      "-0.2508323110901818\n",
      "-0.15864524934838672\n",
      "-0.27598353735170766\n",
      "-0.11368094334810071\n",
      "-0.20394708308902337\n",
      "-0.11323975289296012\n",
      "-0.025774058730187746\n",
      "-0.03898106866721509\n",
      "0.08594371744177745\n",
      "0.04633076076025793\n",
      "-0.05926153789057002\n",
      "-0.052154457403782425\n",
      "0.029433387337628885\n",
      "0.0053413784889663396\n",
      "0.1391421524282861\n",
      "-0.003183245188408265\n",
      "-0.212567338912985\n",
      "-0.24435401534265674\n",
      "-0.20481351829295819\n",
      "-0.15332334288930366\n",
      "-0.2795243009571248\n",
      "-0.2183094557762741\n",
      "-0.2530566474125148\n",
      "-0.28816860673492833\n",
      "-0.24078229379984092\n",
      "-0.2524368572923583\n",
      "-0.16473227071464833\n",
      "-0.2655823227049391\n",
      "-0.17051680348853127\n",
      "-0.19138219413216725\n",
      "-0.1024364873397973\n",
      "-0.06154128891939202\n",
      "-0.2565068197657132\n",
      "-0.2820677731257809\n",
      "-0.2772679158277974\n",
      "-0.3923040812891613\n",
      "-0.42381290128540955\n",
      "-0.4304573403127069\n",
      "-0.4169298599486043\n",
      "-0.3727196408960159\n",
      "-0.34715793824153385\n",
      "-0.28853535252409157\n",
      "-0.44644886901758535\n",
      "-0.39001595569340214\n",
      "-0.4361566533749136\n",
      "-0.40428377622625383\n",
      "-0.4323186269264302\n",
      "-0.3894197965626542\n",
      "-0.32714993969957323\n",
      "-0.34088414341174733\n",
      "-0.33142599052492766\n",
      "-0.2293091710579662\n",
      "-0.2100183801372746\n",
      "-0.15474008404943335\n",
      "-0.2216106246893985\n",
      "-0.27060947722178097\n",
      "-0.24225250010998242\n",
      "-0.35554697192889173\n",
      "-0.328411574404202\n",
      "-0.2907880154729961\n",
      "-0.29767234529435477\n",
      "-0.29404563259919425\n",
      "-0.32427726550876596\n",
      "-0.3412305750535577\n",
      "-0.2731884557717635\n",
      "-0.27991192382608715\n",
      "-0.343282911085688\n",
      "-0.3793688952730355\n",
      "-0.38063515149441124\n",
      "-0.337576472964365\n",
      "-0.2717274461771763\n",
      "-0.3869478100701461\n",
      "-0.4547946438123092\n",
      "-0.3342586657139796\n",
      "-0.3299280916849749\n",
      "-0.2865394160833206\n",
      "-0.31425783606423574\n",
      "-0.3910256856114717\n",
      "-0.42207680011626464\n",
      "-0.3676167197520643\n",
      "-0.4262396661941517\n",
      "-100\n",
      "Episode 8 avg reward -121.80567360721932\n",
      "-0.11784720170575436\n",
      "-0.15862827961186607\n",
      "-0.05191857959842789\n",
      "-0.09056567745649602\n",
      "-0.21128545674105859\n",
      "-0.13731694087033963\n",
      "-0.14343887123862833\n",
      "-0.18504829888835567\n",
      "-0.24558853544424472\n",
      "-0.218669089161504\n",
      "-0.1770899992150255\n",
      "-0.18545234036094205\n",
      "-0.09211212692625384\n",
      "-0.15296307119656555\n",
      "-0.06177655616129206\n",
      "-0.15075743217932558\n",
      "-0.23355984503286292\n",
      "-0.20705654694551073\n",
      "-0.15761711838602954\n",
      "-0.21884900284562636\n",
      "-0.28350945794051696\n",
      "-0.17002461191996326\n",
      "-0.20208357989450995\n",
      "-0.1697969536580626\n",
      "-0.21907641544152714\n",
      "-0.27767384439950776\n",
      "-0.2281137149048644\n",
      "-0.07265081554957624\n",
      "-0.1526166823925414\n",
      "-0.19675481861491476\n",
      "-0.12506841436494484\n",
      "-0.1863446357698501\n",
      "-0.1493813492917108\n",
      "-0.2511106535022035\n",
      "-0.29057435664122566\n",
      "-0.27719716700412167\n",
      "-0.2301905006255787\n",
      "-0.16296237260295676\n",
      "-0.2059724400976662\n",
      "-0.1684111823849628\n",
      "-0.06780200986121099\n",
      "0.08766676323196917\n",
      "0.12830374050218082\n",
      "0.15442020638756818\n",
      "0.15870774000909907\n",
      "0.12728229554230308\n",
      "0.1718140123687262\n",
      "0.11080626310060007\n",
      "0.14932554012842822\n",
      "0.20178266556781566\n",
      "0.21155144092803596\n",
      "0.23555664124573275\n",
      "0.23026289024341176\n",
      "0.25577392290608686\n",
      "0.2823871484275793\n",
      "0.2319843238477034\n",
      "0.26060612488906043\n",
      "0.21307007251575782\n",
      "0.3379751160150295\n",
      "0.3707600539405247\n",
      "0.22930226324394842\n",
      "0.2745355202195264\n",
      "0.32883602114976107\n",
      "0.3148520910464225\n",
      "0.3016612999035925\n",
      "0.29499851722459347\n",
      "0.300607062576933\n",
      "0.3955148362363379\n",
      "0.45319849368005893\n",
      "0.29158892693109945\n",
      "0.32349766274556474\n",
      "0.40349038158704126\n",
      "0.39582791662358047\n",
      "0.48691456892158624\n",
      "0.4175650231466424\n",
      "0.018917677089367434\n",
      "-0.08086488439390034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10459906782016842\n",
      "-0.21858963092266212\n",
      "-0.2365423255624059\n",
      "-0.25224915912394363\n",
      "-0.15948412681328306\n",
      "-0.26241794193026563\n",
      "-0.25491213949521263\n",
      "-0.08969879406509082\n",
      "-0.11151492380657459\n",
      "-100\n",
      "Episode 9 avg reward -99.90438471663836\n",
      "-0.15002271647479296\n",
      "-0.158302053207706\n",
      "-0.1253735021299172\n",
      "-0.16554008990416674\n",
      "-0.15540752462546148\n",
      "-0.11186853756870747\n",
      "-0.09286808134447017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6470913dd6dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" avg reward \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-71e7a44c6465>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#start the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-71e7a44c6465>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time_step)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-71e7a44c6465>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, time_step)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mloss_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "avg_rewards = []\n",
    "max_action = float(env.action_space.high[0])\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.1\n",
    "rewards = []\n",
    "env.observation_space.shape[0]\n",
    "agent = Agent(env, max_action)\n",
    "for episode in range(50):\n",
    "    \n",
    "    episode_rewards = agent.run_episode()\n",
    "    \n",
    "    print('Episode '+str(episode) + \" avg reward \"+ str(np.sum(episode_rewards)))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
