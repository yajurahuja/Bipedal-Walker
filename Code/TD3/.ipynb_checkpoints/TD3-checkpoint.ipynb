{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from collections import deque\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x)) \n",
    "        return x\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l5 = nn.Linear(400, 300)\n",
    "        self.l6 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "\n",
    "        x2 = F.relu(self.l4(xu))\n",
    "        x2 = F.relu(self.l5(x2))\n",
    "        x2 = self.l6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        xu = xu.view(xu.size(0), -1)\n",
    "        x1 = F.relu(self.l1(xu))\n",
    "        x1 = F.relu(self.l2(x1))\n",
    "        x1 = self.l3(x1)\n",
    "        return x1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.Memory = []\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        step = (state, action, reward, next_state, done)\n",
    "        self.Memory.append(step)\n",
    "    \n",
    "    def get_mini_batch(self, batch_size):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        batch = random.sample(self.Memory, batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, max_action, actor_alpha = 1e-3, critic_alpha = 1e-3 ,gamma = 0.99, tau = 0.005, noise_c = 1.0, d = 2, sigma = 1, batch_size = 100):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_c = noise_c\n",
    "        self.actor_alpha = actor_alpha\n",
    "        self.critic_alpha = critic_alpha\n",
    "        self.max_action = max_action\n",
    "        self.d = d\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma \n",
    "        self.sigma = sigma\n",
    "        self.current_state = None\n",
    "        \n",
    "        #Actor Network\n",
    "        self.actor = Actor(env.observation_space.shape[0], env.action_space.shape[0], self.max_action)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.act_opt = optim.Adam(self.actor.parameters(), lr=actor_alpha)\n",
    "        \n",
    "        #Critic Network\n",
    "        self.critic = Critic(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.crit_opt = optim.Adam(self.critic.parameters(), lr=critic_alpha)\n",
    "        \n",
    "        #Relay Buffer\n",
    "        self.replay_buffer = Buffer()\n",
    "        \n",
    "    def step(self, time_step):\n",
    "        action = self.actor.forward(torch.FloatTensor(self.current_state))\n",
    "        action = (action.detach().numpy().flatten() + np.random.normal(0, self.sigma, self.env.action_space.shape[0]))\n",
    "        action.clip(self.env.action_space.low, self.env.action_space.high)\n",
    "        #print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        self.current_state = next_state\n",
    "        self.replay_buffer.add(self.current_state, action, reward, next_state, done)\n",
    "        if self.replay_buffer.get_size() > self.batch_size:\n",
    "            self.update(time_step)\n",
    "        return reward, done\n",
    "   \n",
    "\n",
    "    def update(self, time_step):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.get_mini_batch(self.batch_size) \n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(1-np.array(dones))\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        #print(next_actions.detach().numpy().shape)\n",
    "        \n",
    "        noise = torch.FloatTensor(actions).data.normal_(0, 1) #tensor of size BATCH_SIZE x ACTION_SIZE\n",
    "        noise = noise.clamp(-self.noise_c, self.noise_c)\n",
    "#         print(type(noise), noise.shape)\n",
    "        next_actions = (next_actions + noise).clamp(-self.max_action, self.max_action)\n",
    "        #print(next_states.shape, next_actions.shape)\n",
    "        #update critic\n",
    "        q_1, q_2 = self.critic_target.forward(next_states, next_actions)\n",
    "        y = rewards + (self.gamma * torch.min(q_1, q_2)).detach()\n",
    "        \n",
    "        current_q1, current_q2 = self.critic.forward(states, actions)\n",
    "        loss_c = F.mse_loss(y, current_q1) + F.mse_loss(y, current_q2)\n",
    "        \n",
    "        self.crit_opt.zero_grad()\n",
    "        loss_c.backward()\n",
    "        self.crit_opt.step()\n",
    "        \n",
    "        if time_step % self.d == 0:\n",
    "            #update actor\n",
    "            action = self.actor.forward(states)\n",
    "            loss_a = -self.critic.Q1(states, action).mean() #compute loss\n",
    "            self.act_opt.zero_grad()\n",
    "            loss_a.backward()\n",
    "            \n",
    "            self.act_opt.step()\n",
    "            self.target_update(self.critic, self.critic_target)\n",
    "            self.target_update(self.actor, self.actor_target)\n",
    "\n",
    "    def target_update(self, model, model_target):\n",
    "        for theta, theta_ in zip(model.parameters(), model_target.parameters()):\n",
    "            update = self.tau * theta + (1 - self.tau)*theta_\n",
    "            theta_.data.copy_(update)\n",
    "            \n",
    "    def run_episode(self, current_state):\n",
    "        self.current_state = current_state\n",
    "        episode_done = False\n",
    "        self.env.render() #start the episode\n",
    "        rewards = []\n",
    "        timesteps = 0\n",
    "        while episode_done is False:\n",
    "            reward, episode_done = self.step(timesteps)\n",
    "            avg_rewards.append(reward)\n",
    "            timesteps = timesteps + 1\n",
    "\n",
    "        return rewards\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "env.reset()\n",
    "env.observation_space.shape[0] + env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:62: UserWarning: Using a target size (torch.Size([100, 1])) that is different to the input size (torch.Size([100, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 avg reward nan\n",
      "Episode 1 avg reward nan\n",
      "Episode 2 avg reward nan\n",
      "Episode 3 avg reward nan\n",
      "Episode 4 avg reward nan\n",
      "Episode 5 avg reward nan\n",
      "Episode 6 avg reward nan\n",
      "Episode 7 avg reward nan\n",
      "Episode 8 avg reward nan\n",
      "Episode 9 avg reward nan\n",
      "Episode 10 avg reward nan\n",
      "Episode 11 avg reward nan\n",
      "Episode 12 avg reward nan\n",
      "Episode 13 avg reward nan\n",
      "Episode 14 avg reward nan\n",
      "Episode 15 avg reward nan\n",
      "Episode 16 avg reward nan\n",
      "Episode 17 avg reward nan\n",
      "Episode 18 avg reward nan\n",
      "Episode 19 avg reward nan\n",
      "Episode 20 avg reward nan\n",
      "Episode 21 avg reward nan\n",
      "Episode 22 avg reward nan\n",
      "Episode 23 avg reward nan\n",
      "Episode 24 avg reward nan\n",
      "Episode 25 avg reward nan\n",
      "Episode 26 avg reward nan\n",
      "Episode 27 avg reward nan\n",
      "Episode 28 avg reward nan\n",
      "Episode 29 avg reward nan\n",
      "Episode 30 avg reward nan\n",
      "Episode 31 avg reward nan\n",
      "Episode 32 avg reward nan\n",
      "Episode 33 avg reward nan\n",
      "Episode 34 avg reward nan\n",
      "Episode 35 avg reward nan\n",
      "Episode 36 avg reward nan\n",
      "Episode 37 avg reward nan\n",
      "Episode 38 avg reward nan\n",
      "Episode 39 avg reward nan\n",
      "Episode 40 avg reward nan\n",
      "Episode 41 avg reward nan\n",
      "Episode 42 avg reward nan\n",
      "Episode 43 avg reward nan\n",
      "Episode 44 avg reward nan\n",
      "Episode 45 avg reward nan\n",
      "Episode 46 avg reward nan\n",
      "Episode 47 avg reward nan\n",
      "Episode 48 avg reward nan\n",
      "Episode 49 avg reward nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = []\n",
    "avg_rewards = []\n",
    "max_action = float(env.action_space.high[0])\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "NOISE = 0.2\n",
    "NOISE_CLIP = 0.5\n",
    "EXPLORE_NOISE = 0.1\n",
    "rewards = []\n",
    "env.observation_space.shape[0]\n",
    "agent = Agent(env, max_action)\n",
    "for episode in range(50):\n",
    "    \n",
    "    episode_rewards = agent.run_episode(event.reset())\n",
    "    \n",
    "    print('Episode '+str(episode) + \" avg reward \"+ str(np.mean(episode_rewards)))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
